{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network\n",
    "\n",
    "## 1. Data Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We could initialize a dataloader instance to get what it is like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the DataLoader\n",
    "from src.data.DataLoader import DataLoader\n",
    "\n",
    "# Assuming 'data.csv' is in a folder named 'dataset/data' in the src directory\n",
    "file_path = 'dataset/data/data.csv'\n",
    "\n",
    "# Create an instance of DataLoader\n",
    "data_loader_0 = DataLoader(file_path, shuffle=True)  # setting shuffle to True for testing\n",
    "\n",
    "# Load the data\n",
    "data = data_loader_0.load_data()\n",
    "\n",
    "# Print the first few rows of the data to verify loading\n",
    "print(data.head())\n",
    "\n",
    "# Print what type of data is returned\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data_loader is now created, we initialize a data_processor to handle the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes\n",
    "from src.data.DataProcessor import DataProcessor\n",
    "\n",
    "# Load data using DataLoader (if not already loaded)\n",
    "data = data_loader_0.load_data()\n",
    "\n",
    "# Create an instance of DataProcessor\n",
    "# Assuming normalization is desired and the default split proportions are fine\n",
    "data_processor_0 = DataProcessor(data_loader_0, normalize=True)\n",
    "\n",
    "# Process the data to get train, validation, and test sets\n",
    "train_data, validate_data, test_data = data_processor_0.process_data()\n",
    "\n",
    "# Optionally, print the shapes of the datasets to verify everything is as expected\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", validate_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using BaseDataset to directly create splited and normalized datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.BaseDataset import BaseDataset\n",
    "\n",
    "# Assume file_path is correctly defined relative to the script running this code\n",
    "dataset = BaseDataset('dataset/data/data.csv', shuffle=True, normalize=True)\n",
    "train_data, validate_data, test_data = dataset.prepare_data()\n",
    "\n",
    "# Optionally, directly access data from the dataset object later\n",
    "print(dataset.train_data.head())\n",
    "print(dataset.validate_data.head())\n",
    "print(dataset.test_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using matplotlib to plot SOME of the data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.Utils_Data import plot_all_histograms\n",
    "from src.data.Utils_Data import plot_2d_data\n",
    "\n",
    "# Assuming `df_2D` is the DataFrame containing your 2D dataset\n",
    "plot_2d_data(dataset.train_data, feature_columns=['x1', 'x2'], label_column='target')\n",
    "\n",
    "# Plotting the histogram of train_data for 'x1'\n",
    "plot_all_histograms(dataset.train_data, exclude_columns=['target'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling Procedure\n",
    "### 2.1 Forward passing\n",
    "- After implementing BaseNetwork and the inherited ClassifierNetwork, we can test its forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.ClassifierNetwork import Classifier\n",
    "# Get the number of features from the dataset\n",
    "num_features = dataset.train_data.shape[1] - 1  # Subtract 1 for the target column\n",
    "# Assuming you have a ClassifierNetwork class already imported and ready\n",
    "classifier = Classifier(num_features)  # Initialize with 2 features as your dataset suggests,\n",
    "\n",
    "# Extract features and targets from prepared data\n",
    "X_train = train_data.drop('target', axis=1).values\n",
    "y_train = train_data['target'].values\n",
    "\n",
    "X_val = validate_data.drop('target', axis=1).values\n",
    "y_val = validate_data['target'].values\n",
    "\n",
    "X_test = test_data.drop('target', axis=1).values\n",
    "y_test = test_data['target'].values\n",
    "\n",
    "# Perform a forward pass with the training data\n",
    "train_preds = classifier.forward(X_train)\n",
    "print(\"Training Predictions Shape:\\n\", train_preds.shape)\n",
    "print(\"Some Training Predictions:\\n\", train_preds[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculating Loss\n",
    "- Since the forward pass works correctly, we perform the loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.LossFunction import BCE\n",
    "# Assuming you have a BCE class already implemented\n",
    "loss_func = BCE()\n",
    "\n",
    "# Calculate loss for training data\n",
    "train_loss = loss_func.forward(train_preds, y_train)\n",
    "print(f\"Training Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculating Gradients\n",
    "- Now we should got back and calculate the gradients for all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.Utils_Model import plot_decision_boundary\n",
    "# Step 1: Forward Pass (Already done above)\n",
    "# Step 2: Compute Loss Gradient\n",
    "dloss = loss_func.backward(train_preds, y_train)\n",
    "\n",
    "# Step 3: Backward Pass\n",
    "gradients = classifier.backward(dloss)\n",
    "\n",
    "# Step 4: Print Gradients\n",
    "print(\"Gradients of the weights:\\n\", gradients)\n",
    "plot_decision_boundary(dataset.validate_data, classifier=classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Batch Gradient Descent\n",
    "- After Computing The Whole Network and Averaging to a mean Gradient, we can now update weights and see the train loss changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.Optimizer import Optimizer\n",
    "\n",
    "for i in range(4):\n",
    "    for x in range(20):\n",
    "        for y in range(x):\n",
    "            # Use the optimizer to update the model's weights based on the gradients\n",
    "            optimizer = Optimizer(classifier, learning_rate=0.01)\n",
    "            optimizer.step(gradients)\n",
    "\n",
    "            # Re-evaluate the training loss after updating the weights\n",
    "            train_preds_updated = classifier.forward(X_train)\n",
    "            updated_train_loss = loss_func.forward(train_preds_updated, y_train)\n",
    "    plot_decision_boundary(dataset.validate_data, classifier=classifier)\n",
    "    print(f\"Updated Training Loss: \\\n",
    "        {loss_func.forward(classifier.forward(X_train), y_train):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Solver import Solver\n",
    "\n",
    "solver = Solver(model=classifier, data={'X_train': X_train, 'y_train': y_train,\n",
    "                                        'X_val': X_val, 'y_val': y_val},\n",
    "                loss_func=loss_func, learning_rate=0.001, batch_size=32)\n",
    "solver.train(epochs=200)\n",
    "plot_decision_boundary(dataset.validate_data, classifier=classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
